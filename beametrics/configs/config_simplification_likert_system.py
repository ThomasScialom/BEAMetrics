import numpy as np
import pandas as pd
import ast
from beametrics.configs.config_base import ConfigBase
from beametrics.metrics.metric_reporter import _DEFAULT_METRIC_NAMES_SRC

class SimplificationLikertSystem(ConfigBase):
    def __init__(self):

        file_name = 'simplification_absolute_ratings.csv'
        file_name_processed = 'processed.simplification.likert_system'
        metric_names = _DEFAULT_METRIC_NAMES_SRC
        metric_names = metric_names + ('sari',)

        language = "en"
        task = "simplification"
        nb_refs = 10

        dimensions = ('simplicity', 'meaning', 'fluency')

        dimensions_definitions = {
            'simplicity': "to what extent is the evaluated text easier to read and understand?",
            'meaning': "how well the evaluated text expresses the original meaning?",
            'fluency': "how fluent is the evaluated text?"
        }

        scale = "likert"

        sampled_from = "https://www.aclweb.org/anthology/2020.acl-main.424/"

        citation = """@inproceedings{alva2020asset, 
                            title={ASSET: A Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations},
                            author={Alva-Manchego, Fernando and Martin, Louis and Bordes, Antoine and Scarton, Carolina and Sagot, Beno{\^\i}t and Specia, Lucia},
                            booktitle={ACL 2020-58th Annual Meeting of the Association for Computational Linguistics},
                            year={2020}}"""

        additional_comments = "The evaluated texts are generated by automatic systems and evaluated on a likert scale from 1 to 5."

        super().__init__(
            file_name=file_name,
            file_name_processed=file_name_processed,
            metric_names=metric_names,
            language=language,
            task=task,
            nb_refs=nb_refs,
            dimensions=dimensions,
            dimensions_definitions=dimensions_definitions,
            scale=scale,
            sampled_from=sampled_from,
            citation=citation,
            additional_comments=additional_comments
        )

    def format_file(
        self,
        path
    ):

        system_or_human = 'system'
        do_avg_raters = True
        normalise_rate = True

        df = pd.read_csv(path)

        d_worker_id = {}
        for i, row in df.iterrows():
            if row['worker_id'] not in d_worker_id:
                d_worker_id[row['worker_id']] = []
            d_worker_id[row['worker_id']].append(row['rating'])

        d_data = {}
        for i, row in df.iterrows():

            if row['simplification_type'] != system_or_human:
                continue

            ex_id = f"{row['sentence_id']}_{row['simplification']}"
            if ex_id not in d_data:
                d_data[ex_id] = {'source': row['source'],
                                 'hypothesis': row['simplification'],
                                 'references': ast.literal_eval(row['references']),
                                 }

            aspect = row['aspect']
            assert aspect in self.dimensions, f'Aspect {aspect} is not in the dimensions {self.dimensions}'
            if aspect not in d_data[ex_id]:
                d_data[ex_id][aspect] = []
            rate = row['rating']
            if normalise_rate:
                rate = (rate - np.average(d_worker_id[row['worker_id']])) / np.var(d_worker_id[row['worker_id']])
            d_data[ex_id][aspect].append(rate)

        d_annotators = {dim: [] for dim in self.dimensions}
        if do_avg_raters:
            for k in d_data.keys():
                for dim in self.dimensions:
                    d_annotators[dim].append(d_data[k][dim])
                    d_data[k][dim] = np.average(d_data[k][dim])

        """
        print('Krippendorff alpha: ')
        for dim in dimensions:
            kappa = krippendorff.alpha(np.array(d_annotators[dim]).transpose())
            print(dim, kappa)
        """

        return d_data