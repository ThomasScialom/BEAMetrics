# Table card: RealSum (Rea)

This table card grasp all the information for RealSum. The specific definition of each field is available at this [link](https://github.com/ThomasScialom/BEAMetrics#adding-a-new-dataset).

**Task:** 
`summarization`

**Source Evaluation Set(s):** 
`CNNDM`

**Language(s):** 
`en`

**Evaluated Dimensions:** 
```
- litepyramid_recall: A lightweight sampling-based version of the Pyramid that is crowdsourcable. See Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation
```

**Evaluation Scale:** 
`likert`

**Number Of Evaluated Texts** 
`2500`

**Number Of  References** 
`1`

**Information About The Annotators** 
`Expert annotators working at the Sorbonne University.`

**Additional Information** 
```
100 documents for which 14 abstractive & 11 extractive summaries are annotated following the pyramid method
```

**Data URL:** 
``https://github.com/neulab/REALSumm``

**Citation:** 
```
@inproceedings{Bhandari-2020-reevaluating,
        title = "Re-evaluating Evaluation in Text Summarization",
        author = "Bhandari, Manik  and Narayan Gour, Pranav  and Ashfaq, Atabak  and  Liu, Pengfei and Neubig, Graham ",
        booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
        year = "2020"
}
```
