# Table card: WebNLG-Eval (Web)

This table card grasp all the information for WebNLG-Eval. The specific definition of each field is available at this [link](https://github.com/ThomasScialom/BEAMetrics#adding-a-new-dataset).

**Task:** 
`data2text`

**Source Evaluation Set(s):** 
`WebNLG`

**Language(s):** 
`en`

**Evaluated Dimensions:** 
```
- fluency: Rate the fluency of the text: Does the text sound fluent and natural? (1 - Not fluent, 2 - Medium, 3 - Fluent)
- grammar: Rate the grammar and the spelling of the text: Is the text grammatical (no spelling or grammatical errors)? (1 - Ungrammatical, 2 - Medium, 3 - Grammatical)
- semantics: Does the text correctly represent the meaning in the data? (1 - Incorrectly, 2 - Medium, 3 - Correctly)
```

**Evaluation Scale:** 
`likert 1-3`

**Number Of Evaluated Texts** 
`2007`

**Number Of  References** 
`3`

**Information About The Annotators** 
`English Speaker from https://www.crowdflower.com/ (See more on [the corresponding paper](English Speaker from https://www.crowdflower.com/)`

**Additional Information** 
```
The evaluated texts are generated by automatic systems on WebNLG and evaluated on a likert scale from 1 to 3.
```

**Data URL:** 
``https://webnlg-challenge.loria.fr/files/webnlg-human-evaluation-results.pdf``

**Citation:** 
```
@article{shimorina2019webnlg,
        title={WebNLG Challenge: Human Evaluation Results},
        author={Shimorina, Anastasia and Gardent, Claire and Narayan, Shashi and Perez-Beltrachini, Laura},
        year={2019}
}
```
